{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIPTION:\n",
    "    This example shows how to create embeddings from Employee's CV files in the /data/CV/ folder and save it to a FAISS index.\n",
    "    Then query these documents and get an answer using OpenAI GPT3.5 with chat\n",
    "### REQUIREMENTS:\n",
    "    Create an .env file with your OpenAI API key and save it in the root directory of this project.\n",
    "\n",
    "  For more information about Faiss index, see:\n",
    "      https://github.com/facebookresearch/faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TokenTextSplitter\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTANT!!\n",
    "# you only need to run this once to create the embeddings, then you can use the faiss_index for retrieval\n",
    "# you can use the already created faiss_index in the dbs folder - ./dbs/CV/faiss_index\n",
    "\n",
    "# create embeddings from Employee's CV files in the /data/CV/ folder and save it to a FAISS index.\n",
    "utils.init_OpenAI()\n",
    "embeddings = utils.init_embeddings()\n",
    "loader = DirectoryLoader('./data/CV/', glob=\"*.txt\", loader_cls=TextLoader)\n",
    "documents = loader.load()\n",
    "text_splitter = TokenTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "db = FAISS.from_documents(documents=docs, embedding=embeddings)\n",
    "db.save_local(\"./dbs/CV/faiss_index\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the llm and the vectorstore retriever\n",
    "llm = utils.init_llm()\n",
    "embeddings = utils.init_embeddings()\n",
    "\n",
    "vectorStore = FAISS.load_local(\"./dbs/CV/faiss_index\", embeddings)\n",
    "vs_retriever = vectorStore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(qa, chat_history, query):\n",
    "    result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "    chat_history = [(query, result[\"answer\"])]\n",
    "    print(result[\"answer\"])\n",
    "    return chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "QUESTION_PROMPT = PromptTemplate.from_template(\"\"\"Given the following conversation and a follow up question, rephrase the follow up question to be a standalone question.\n",
    "    Chat History:\n",
    "    {chat_history}\n",
    "    Follow Up Input: {question}\n",
    "    Standalone question:\"\"\"\n",
    ")\n",
    "qa = ConversationalRetrievalChain.from_llm(llm=llm,\n",
    "                                retriever=vs_retriever,\n",
    "                                condense_question_prompt=QUESTION_PROMPT,\n",
    "                                return_source_documents=True,\n",
    "                                verbose=False)\n",
    "chat_history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Yes, there are two solution architects. Christopher L. Hall and Jordan Zhu are both solution architects.\n",
      "\"\"\"<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "chat_history = ask_question(qa, chat_history, \"Are there any solution architects?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Requests to the Creates a completion for the chat message Operation under Azure OpenAI API version 2023-03-15-preview have exceeded call rate limit of your current OpenAI S0 pricing tier. Please retry after 1 second. Please go here: https://aka.ms/oai/quotaincrease if you would like to further increase the default rate limit..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both Christopher L. Hall and Jordan Zhu are proficient in cloud technologies. Christopher L. Hall is an AWS-certified big data solution architect with experience in designing and deploying cloud data protection architecture for Fortune 500 companies using Amazon Web Services. Jordan Zhu has experience as a Solutions Architect at Stripe and a Software Engineer at Amazon, both of which are companies that heavily utilize cloud technologies.\n"
     ]
    }
   ],
   "source": [
    "chat_history = ask_question(qa, chat_history, \"is any of them proficient in cloud technologies?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is no information provided about Jordan Zhu's experience or qualifications in big data technologies. Christopher L. Hall is an AWS-certified big data solution architect with 4+ years of experience driving information management strategy. His expertise is in cloud data protection and he has experience with AWS cloud infrastructure. However, there is no information provided about his proficiency in other big data technologies such as Hadoop, Spark, and Hive.\n"
     ]
    }
   ],
   "source": [
    "chat_history = ask_question(qa, chat_history, \"do they know big data technologies?\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
