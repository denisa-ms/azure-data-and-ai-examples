{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DESCRIPTION:\n",
    "    This example shows how to use Azure OpenAI GPT3.5 to interpret image tags extracted using Azure Cognitive Services\n",
    "\n",
    "### REQUIREMENTS:\n",
    "    Create an .env file with your OpenAI API key and save it in the root directory of this project.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import os\n",
    "import azure.ai.vision as visionsdk\n",
    "\n",
    "def analyze_image(image_url):\n",
    "    service_options = visionsdk.VisionServiceOptions(utils.AZURE_COMPUTER_VISION_ENDPOINT, utils.AZURE_COMPUTER_VISION_KEY)\n",
    "\n",
    "    # Specify the image file on disk to analyze. sample.jpg is a good example to show most features\n",
    "    # vision_source = visionsdk.VisionSource(filename=\"sample.jpg\")\n",
    "\n",
    "    # Or, instead of the above, specify a publicly accessible image URL to analyze. For example:\n",
    "    vision_source = visionsdk.VisionSource(url=image_url)\n",
    "\n",
    "    analysis_options = visionsdk.ImageAnalysisOptions()\n",
    "\n",
    "    # Mandatory. You must set one or more features to analyze. Here we use the full set of features.\n",
    "    # Note that \"CAPTION\" and \"DENSE_CAPTIONS\" are only supported in Azure GPU regions (East US, France Central,\n",
    "    # Korea Central, North Europe, Southeast Asia, West Europe, West US). Remove \"CAPTION\" and \"DENSE_CAPTIONS\"\n",
    "    # from the list below if your Computer Vision key is not from one of those regions.\n",
    "    analysis_options.features = (\n",
    "        # visionsdk.ImageAnalysisFeature.CROP_SUGGESTIONS |\n",
    "        visionsdk.ImageAnalysisFeature.CAPTION |\n",
    "        visionsdk.ImageAnalysisFeature.DENSE_CAPTIONS |\n",
    "        visionsdk.ImageAnalysisFeature.OBJECTS |\n",
    "        visionsdk.ImageAnalysisFeature.PEOPLE |\n",
    "        visionsdk.ImageAnalysisFeature.TEXT |\n",
    "        visionsdk.ImageAnalysisFeature.TAGS\n",
    "    )\n",
    "\n",
    "    # Optional, and only relevant when you select ImageAnalysisFeature.CROP_SUGGESTIONS.\n",
    "    # Define one or more aspect ratios for the desired cropping. Each aspect ratio needs\n",
    "    # to be in the range [0.75, 1.8]. If you do not set this, the service will return one\n",
    "    # crop suggestion with the aspect ratio it sees fit.\n",
    "    # analysis_options.cropping_aspect_ratios = [0.9, 1.33]\n",
    "\n",
    "    # Optional. Default is \"en\" for English. See https://aka.ms/cv-languages for a list of supported\n",
    "    # language codes and which visual features are supported for each language.\n",
    "    analysis_options.language = \"en\"\n",
    "    analysis_options.model_version = \"latest\"\n",
    "    # Set this to \"true\" to get a gender neutral caption (the default is \"false\").\n",
    "    analysis_options.gender_neutral_caption = True\n",
    "\n",
    "    # Create the image analyzer object\n",
    "    image_analyzer = visionsdk.ImageAnalyzer(service_options, vision_source, analysis_options)\n",
    "\n",
    "    # This call creates the network connection and blocks until Image Analysis results\n",
    "    # return (or an error occurred). Note that there is also an asynchronous (non-blocking)\n",
    "    # version of this method: image_analyzer.analyze_async().\n",
    "    result = image_analyzer.analyze()\n",
    "\n",
    "    # Checks result.\n",
    "    if result.reason == visionsdk.ImageAnalysisResultReason.ANALYZED:\n",
    "\n",
    "        print(\" Image height: {}\".format(result.image_height))\n",
    "        print(\" Image width: {}\".format(result.image_width))\n",
    "        print(\" Model version: {}\".format(result.model_version))\n",
    "\n",
    "        if result.caption is not None:\n",
    "            print(\" Caption:\")\n",
    "            print(\"   '{}', Confidence {:.4f}\".format(result.caption.content, result.caption.confidence))\n",
    "\n",
    "        if result.dense_captions is not None:\n",
    "            print(\" Dense Captions:\")\n",
    "            for caption in result.dense_captions:\n",
    "                print(\"   '{}', {}, Confidence: {:.4f}\".format(caption.content, caption.bounding_box, caption.confidence))\n",
    "\n",
    "        if result.objects is not None:\n",
    "            print(\" Objects:\")\n",
    "            for object in result.objects:\n",
    "                print(\"   '{}', {}, Confidence: {:.4f}\".format(object.name, object.bounding_box, object.confidence))\n",
    "\n",
    "        if result.tags is not None:\n",
    "            print(\" Tags:\")\n",
    "            for tag in result.tags:\n",
    "                print(\"   '{}', Confidence {:.4f}\".format(tag.name, tag.confidence))\n",
    "\n",
    "        if result.people is not None:\n",
    "            print(\" People:\")\n",
    "            for person in result.people:\n",
    "                print(\"   {}, Confidence {:.4f}\".format(person.bounding_box, person.confidence))\n",
    "\n",
    "        if result.crop_suggestions is not None:\n",
    "            print(\" Crop Suggestions:\")\n",
    "            for crop_suggestion in result.crop_suggestions:\n",
    "                print(\"   Aspect ratio {}: Crop suggestion {}\"\n",
    "                      .format(crop_suggestion.aspect_ratio, crop_suggestion.bounding_box))\n",
    "\n",
    "        if result.text is not None:\n",
    "            print(\" Text:\")\n",
    "            for line in result.text.lines:\n",
    "                points_string = \"{\" + \", \".join([str(int(point)) for point in line.bounding_polygon]) + \"}\"\n",
    "                print(\"   Line: '{}', Bounding polygon {}\".format(line.content, points_string))\n",
    "                for word in line.words:\n",
    "                    points_string = \"{\" + \", \".join([str(int(point)) for point in word.bounding_polygon]) + \"}\"\n",
    "                    print(\"     Word: '{}', Bounding polygon {}, Confidence {:.4f}\"\n",
    "                          .format(word.content, points_string, word.confidence))\n",
    "\n",
    "        result_details = visionsdk.ImageAnalysisResultDetails.from_result(result)\n",
    "        print(\" Result details:\")\n",
    "        print(\"   Image ID: {}\".format(result_details.image_id))\n",
    "        print(\"   Result ID: {}\".format(result_details.result_id))\n",
    "        print(\"   Connection URL: {}\".format(result_details.connection_url))\n",
    "        print(\"   JSON result: {}\".format(result_details.json_result))\n",
    "\n",
    "    else:\n",
    "        error_details = visionsdk.ImageAnalysisErrorDetails.from_result(result)\n",
    "        print(\" Analysis failed.\")\n",
    "        print(\"   Error reason: {}\".format(error_details.reason))\n",
    "        print(\"   Error code: {}\".format(error_details.error_code))\n",
    "        print(\"   Error message: {}\".format(error_details.message))\n",
    "        print(\" Did you set the computer vision endpoint and key?\")\n",
    "\n",
    "    return result_details.json_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Image height: 432\n",
      " Image width: 648\n",
      " Model version: 2023-02-01-preview\n",
      " Caption:\n",
      "   'a person wearing a mask sitting at a table with a laptop', Confidence 1.0000\n",
      " Dense Captions:\n",
      "   'a person wearing a mask sitting at a table with a laptop', Rectangle(x=0, y=0, w=648, h=432), Confidence: 1.0000\n",
      "   'a person using a laptop', Rectangle(x=220, y=289, w=144, h=73), Confidence: 1.0000\n",
      "   'a person wearing a colorful face mask', Rectangle(x=285, y=178, w=202, h=249), Confidence: 1.0000\n",
      "   'a green chair in a room', Rectangle(x=463, y=160, w=117, h=184), Confidence: 1.0000\n",
      "   'a close-up of a person's hand', Rectangle(x=217, y=162, w=109, h=180), Confidence: 1.0000\n",
      "   'a person sitting in a chair', Rectangle(x=418, y=320, w=105, h=109), Confidence: 1.0000\n",
      "   'a blue and green background', Rectangle(x=456, y=163, w=60, h=155), Confidence: 1.0000\n",
      "   'a close-up of a wooden table', Rectangle(x=59, y=318, w=55, h=58), Confidence: 1.0000\n",
      "   'a person sitting at a table', Rectangle(x=287, y=315, w=236, h=113), Confidence: 1.0000\n",
      "   'a close up of a laptop on a table', Rectangle(x=88, y=338, w=354, h=88), Confidence: 1.0000\n",
      " Objects:\n",
      "   'person', Rectangle(x=303, y=194, w=181, h=223), Confidence: 0.7650\n",
      "   'Laptop', Rectangle(x=221, y=289, w=159, h=80), Confidence: 0.5740\n",
      " Tags:\n",
      "   'furniture', Confidence 0.9881\n",
      "   'clothing', Confidence 0.9805\n",
      "   'person', Confidence 0.9484\n",
      "   'houseplant', Confidence 0.9421\n",
      "   'desk', Confidence 0.9157\n",
      "   'indoor', Confidence 0.9050\n",
      "   'computer', Confidence 0.8923\n",
      "   'laptop', Confidence 0.8696\n",
      "   'sitting', Confidence 0.8195\n",
      "   'wall', Confidence 0.7606\n",
      "   'woman', Confidence 0.7446\n",
      "   'table', Confidence 0.6903\n",
      "   'plant', Confidence 0.6417\n",
      "   'using', Confidence 0.5301\n",
      " People:\n",
      "   Rectangle(x=296, y=181, w=196, h=250), Confidence 0.9579\n",
      "   Rectangle(x=2, y=30, w=8, h=23), Confidence 0.0047\n",
      "   Rectangle(x=623, y=182, w=24, h=193), Confidence 0.0030\n",
      " Text:\n",
      "   Line: 'Sample text', Bounding polygon {541, 376, 632, 376, 632, 389, 541, 389}\n",
      "     Word: 'Sample', Bounding polygon {542, 377, 588, 377, 587, 389, 542, 389}, Confidence 0.9920\n",
      "     Word: 'text', Bounding polygon {598, 377, 630, 376, 630, 390, 598, 389}, Confidence 0.9890\n",
      "   Line: 'Hand writing', Bounding polygon {540, 393, 613, 395, 613, 408, 540, 406}\n",
      "     Word: 'Hand', Bounding polygon {540, 394, 569, 394, 569, 407, 540, 407}, Confidence 0.9910\n",
      "     Word: 'writing', Bounding polygon {573, 394, 613, 395, 613, 409, 573, 407}, Confidence 0.9950\n",
      "   Line: '123 456', Bounding polygon {542, 411, 592, 411, 592, 424, 542, 423}\n",
      "     Word: '123', Bounding polygon {542, 412, 561, 411, 561, 424, 542, 424}, Confidence 0.9980\n",
      "     Word: '456', Bounding polygon {568, 411, 590, 412, 590, 424, 568, 424}, Confidence 0.9980\n",
      " Result details:\n",
      "   Image ID: https://aka.ms/azai/vision/image-analysis-sample.jpg\n",
      "   Result ID: 835b74c7-6152-49d4-baef-ac7571f723b5\n",
      "   Connection URL: https://computervisiondenisa.cognitiveservices.azure.com/computervision/imageanalysis:analyze?api-version=2023-02-01-preview&features=tags%2Ccaption%2CdenseCaptions%2Cobjects%2Cpeople%2Cread&gender-neutral-caption=true&language=en&model-version=latest\n",
      "   JSON result: {\"captionResult\":{\"text\":\"a person wearing a mask sitting at a table with a laptop\",\"confidence\":1.0},\"objectsResult\":{\"values\":[{\"boundingBox\":{\"x\":303,\"y\":194,\"w\":181,\"h\":223},\"tags\":[{\"name\":\"person\",\"confidence\":0.765}]},{\"boundingBox\":{\"x\":221,\"y\":289,\"w\":159,\"h\":80},\"tags\":[{\"name\":\"Laptop\",\"confidence\":0.574}]}]},\"readResult\":{\"stringIndexType\":\"TextElements\",\"content\":\"Sample text\\nHand writing\\n123 456\",\"pages\":[{\"height\":432.0,\"width\":648.0,\"angle\":0.5729,\"pageNumber\":1,\"words\":[{\"content\":\"Sample\",\"boundingBox\":[542.0,377.0,588.0,377.0,587.0,389.0,542.0,389.0],\"confidence\":0.992,\"span\":{\"offset\":0,\"length\":6}},{\"content\":\"text\",\"boundingBox\":[598.0,377.0,630.0,376.0,630.0,390.0,598.0,389.0],\"confidence\":0.989,\"span\":{\"offset\":7,\"length\":4}},{\"content\":\"Hand\",\"boundingBox\":[540.0,394.0,569.0,394.0,569.0,407.0,540.0,407.0],\"confidence\":0.991,\"span\":{\"offset\":12,\"length\":4}},{\"content\":\"writing\",\"boundingBox\":[573.0,394.0,613.0,395.0,613.0,409.0,573.0,407.0],\"confidence\":0.995,\"span\":{\"offset\":17,\"length\":7}},{\"content\":\"123\",\"boundingBox\":[542.0,412.0,561.0,411.0,561.0,424.0,542.0,424.0],\"confidence\":0.998,\"span\":{\"offset\":25,\"length\":3}},{\"content\":\"456\",\"boundingBox\":[568.0,411.0,590.0,412.0,590.0,424.0,568.0,424.0],\"confidence\":0.998,\"span\":{\"offset\":29,\"length\":3}}],\"spans\":[{\"offset\":0,\"length\":32}],\"lines\":[{\"content\":\"Sample text\",\"boundingBox\":[541.0,376.0,632.0,376.0,632.0,389.0,541.0,389.0],\"spans\":[{\"offset\":0,\"length\":11}]},{\"content\":\"Hand writing\",\"boundingBox\":[540.0,393.0,613.0,395.0,613.0,408.0,540.0,406.0],\"spans\":[{\"offset\":12,\"length\":12}]},{\"content\":\"123 456\",\"boundingBox\":[542.0,411.0,592.0,411.0,592.0,424.0,542.0,423.0],\"spans\":[{\"offset\":25,\"length\":7}]}]}],\"styles\":[],\"modelVersion\":\"2022-04-30\"},\"denseCaptionsResult\":{\"values\":[{\"text\":\"a person wearing a mask sitting at a table with a laptop\",\"confidence\":1.0,\"boundingBox\":{\"x\":0,\"y\":0,\"w\":648,\"h\":432}},{\"text\":\"a person using a laptop\",\"confidence\":1.0,\"boundingBox\":{\"x\":220,\"y\":289,\"w\":144,\"h\":73}},{\"text\":\"a person wearing a colorful face mask\",\"confidence\":1.0,\"boundingBox\":{\"x\":285,\"y\":178,\"w\":202,\"h\":249}},{\"text\":\"a green chair in a room\",\"confidence\":1.0,\"boundingBox\":{\"x\":463,\"y\":160,\"w\":117,\"h\":184}},{\"text\":\"a close-up of a person's hand\",\"confidence\":1.0,\"boundingBox\":{\"x\":217,\"y\":162,\"w\":109,\"h\":180}},{\"text\":\"a person sitting in a chair\",\"confidence\":1.0,\"boundingBox\":{\"x\":418,\"y\":320,\"w\":105,\"h\":109}},{\"text\":\"a blue and green background\",\"confidence\":1.0,\"boundingBox\":{\"x\":456,\"y\":163,\"w\":60,\"h\":155}},{\"text\":\"a close-up of a wooden table\",\"confidence\":1.0,\"boundingBox\":{\"x\":59,\"y\":318,\"w\":55,\"h\":58}},{\"text\":\"a person sitting at a table\",\"confidence\":1.0,\"boundingBox\":{\"x\":287,\"y\":315,\"w\":236,\"h\":113}},{\"text\":\"a close up of a laptop on a table\",\"confidence\":1.0,\"boundingBox\":{\"x\":88,\"y\":338,\"w\":354,\"h\":88}}]},\"modelVersion\":\"2023-02-01-preview\",\"metadata\":{\"width\":648,\"height\":432},\"tagsResult\":{\"values\":[{\"name\":\"furniture\",\"confidence\":0.9880748987197876},{\"name\":\"clothing\",\"confidence\":0.9805002212524414},{\"name\":\"person\",\"confidence\":0.948422908782959},{\"name\":\"houseplant\",\"confidence\":0.9420685768127441},{\"name\":\"desk\",\"confidence\":0.9156692028045654},{\"name\":\"indoor\",\"confidence\":0.9050061106681824},{\"name\":\"computer\",\"confidence\":0.8922904133796692},{\"name\":\"laptop\",\"confidence\":0.8696370124816895},{\"name\":\"sitting\",\"confidence\":0.819450855255127},{\"name\":\"wall\",\"confidence\":0.7605603933334351},{\"name\":\"woman\",\"confidence\":0.7446085214614868},{\"name\":\"table\",\"confidence\":0.6902506351470947},{\"name\":\"plant\",\"confidence\":0.641657829284668},{\"name\":\"using\",\"confidence\":0.5301232933998108}]},\"peopleResult\":{\"values\":[{\"boundingBox\":{\"x\":296,\"y\":181,\"w\":196,\"h\":250},\"confidence\":0.9578750729560852},{\"boundingBox\":{\"x\":2,\"y\":30,\"w\":8,\"h\":23},\"confidence\":0.004669021349400282},{\"boundingBox\":{\"x\":623,\"y\":182,\"w\":24,\"h\":193},\"confidence\":0.0030183952767401934}]}}\n"
     ]
    }
   ],
   "source": [
    "image_url = \"https://aka.ms/azai/vision/image-analysis-sample.jpg\"\n",
    "analyze_image(image_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
